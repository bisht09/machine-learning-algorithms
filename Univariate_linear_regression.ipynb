{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdWr95ruZp58gGAReTIBaz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bisht09/machine-learning-algorithms/blob/main/Univariate_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s48buKX3JVkg"
      },
      "outputs": [],
      "source": [
        "import math, copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array([1.0, 2.0])\n",
        "y_train = np.array([300.0, 500.0])"
      ],
      "metadata": {
        "id": "2wbG-Sa1MFN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Computing the cost function\n",
        "def compute_cost(x , y , w , b):\n",
        "  \"\"\"\n",
        "  Params: x->numpy list, y->numpy list, w->integer, b->integer\n",
        "  \"\"\"\n",
        "  m = y.shape[0]\n",
        "  cost = 0\n",
        "\n",
        "  for i in range(m):\n",
        "    cost +=  ((w*x[i] + b) - y[i]) ** 2\n",
        "\n",
        "  return 1 / (2* m) * cost"
      ],
      "metadata": {
        "id": "4WIyItqcNER7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#computing gradient for w and b\n",
        "def compute_gradient(x, y, w, b):\n",
        "  \"\"\"\n",
        "  Params: x->numpy list, y->numpy list, w->integer, b->integer\n",
        "  \"\"\"\n",
        "  m = y.shape[0]\n",
        "  gradient_w = 0\n",
        "  gradient_b = 0\n",
        "\n",
        "  for i in range(m):\n",
        "    dj_dw_i = ((w*x[i] + b) - y[i])\n",
        "    gradient_w += dj_dw_i * x[i]\n",
        "    gradient_b += dj_dw_i\n",
        "\n",
        "  return gradient_w / m, gradient_b / m"
      ],
      "metadata": {
        "id": "JeuOLDTuOZB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(x, y, w_in, b_in, learning_rate, epoch, cost_function, gradient_function):\n",
        "  \"\"\"\n",
        "    Performs gradient descent to fit w,b. Updates w,b by taking\n",
        "    num_iters gradient steps with learning rate alpha\n",
        "\n",
        "    Args:\n",
        "      x (ndarray (m,))  : Data, m examples\n",
        "      y (ndarray (m,))  : target values\n",
        "      w_in,b_in (scalar): initial values of model parameters\n",
        "      learning_rate (float):     Learning rate\n",
        "      epoch (int):   number of iterations to run gradient descent\n",
        "      cost_function:     function to call to produce cost\n",
        "      gradient_function: function to call to produce gradient\n",
        "\n",
        "    Returns:\n",
        "      w (scalar): Updated value of parameter after running gradient descent\n",
        "      b (scalar): Updated value of parameter after running gradient descent\n",
        "      J_history (List): History of cost values\n",
        "      p_history (list): History of parameters [w,b]\n",
        "  \"\"\"\n",
        "  J_history = []\n",
        "  p_history = []\n",
        "  b = b_in\n",
        "  w = w_in\n",
        "\n",
        "  for i in range(epoch):\n",
        "    #calculating gradients\n",
        "    dj_dw, dj_db = gradient_function(x,y,w,b)\n",
        "\n",
        "    #update b and w\n",
        "    b = b - learning_rate * dj_db\n",
        "    w = w - learning_rate * dj_dw\n",
        "\n",
        "    if i < 100000:\n",
        "      J_history.append(cost_function(x, y, w, b))\n",
        "      p_history.append([w,b])\n",
        "\n",
        "    #print cost at every 10th interval\n",
        "    if i % math.ceil(epoch/10) == 0:\n",
        "      print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e}, dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}, w: {w: 0.3e}, b:{b: 0.5e}\")\n",
        "\n",
        "  return w, b, J_history, p_history"
      ],
      "metadata": {
        "id": "UyCDqhUXP2T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize parameters\n",
        "w_init = 0\n",
        "b_init = 0\n",
        "# some gradient descent settings\n",
        "iterations = 10000\n",
        "tmp_alpha = 1.0e-2\n",
        "# run gradient descent\n",
        "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, iterations, compute_cost, compute_gradient)\n",
        "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQYjzdv3P763",
        "outputId": "712c4ac9-9560-4212-a02b-2e2ea0ae9553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration    0: Cost 7.93e+04, dj_dw: -6.500e+02, dj_db: -4.000e+02, w:  6.500e+00, b: 4.00000e+00\n",
            "Iteration 1000: Cost 3.41e+00, dj_dw: -3.712e-01, dj_db:  6.007e-01, w:  1.949e+02, b: 1.08228e+02\n",
            "Iteration 2000: Cost 7.93e-01, dj_dw: -1.789e-01, dj_db:  2.895e-01, w:  1.975e+02, b: 1.03966e+02\n",
            "Iteration 3000: Cost 1.84e-01, dj_dw: -8.625e-02, dj_db:  1.396e-01, w:  1.988e+02, b: 1.01912e+02\n",
            "Iteration 4000: Cost 4.28e-02, dj_dw: -4.158e-02, dj_db:  6.727e-02, w:  1.994e+02, b: 1.00922e+02\n",
            "Iteration 5000: Cost 9.95e-03, dj_dw: -2.004e-02, dj_db:  3.243e-02, w:  1.997e+02, b: 1.00444e+02\n",
            "Iteration 6000: Cost 2.31e-03, dj_dw: -9.660e-03, dj_db:  1.563e-02, w:  1.999e+02, b: 1.00214e+02\n",
            "Iteration 7000: Cost 5.37e-04, dj_dw: -4.657e-03, dj_db:  7.535e-03, w:  1.999e+02, b: 1.00103e+02\n",
            "Iteration 8000: Cost 1.25e-04, dj_dw: -2.245e-03, dj_db:  3.632e-03, w:  2.000e+02, b: 1.00050e+02\n",
            "Iteration 9000: Cost 2.90e-05, dj_dw: -1.082e-03, dj_db:  1.751e-03, w:  2.000e+02, b: 1.00024e+02\n",
            "(w,b) found by gradient descent: (199.9929,100.0116)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jzLWwJ9jXiZ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}